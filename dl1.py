# -*- coding: utf-8 -*-
"""DL1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XMe90wdzKbbHjG_GvDYPkp3JxnJ6YqLV
"""

import requests
import zipfile
import io
import os

def download_and_extract_dataset(url, dataset_dir):

    # Ensure the directory exists
    os.makedirs(dataset_dir, exist_ok=True)

    # Download the dataset
    response = requests.get(url)

    # Check if the download was successful
    if response.status_code == 200:
        try:
            # Extract the dataset
            with zipfile.ZipFile(io.BytesIO(response.content)) as z:
                z.extractall(dataset_dir)
            print(f"Dakshina dataset downloaded and extracted to '{dataset_dir}'")
        except zipfile.BadZipFile:
            print(f"Error: The downloaded file is not a valid zip file.")
    else:
        print(f"Failed to download the dataset: {response.status_code} {response.reason}")

# URL for the Dakshina dataset
dataset_url = "https://github.com/google-research-datasets/dakshina/archive/refs/heads/master.zip"

# Directory to save the dataset
dataset_directory = "dakshina_dataset"

# Call the function to download and extract the dataset
download_and_extract_dataset(dataset_url, dataset_directory)

from google.colab import files

def upload_dataset():

    print("Please upload your dataset file.")
    uploaded = files.upload()

    # Check if a file has been uploaded
    if uploaded:
        file_name = list(uploaded.keys())[0]
        print(f"Dataset '{file_name}' uploaded successfully.")
        return file_name
    else:
        print("No file uploaded.")
        return None

# Call the function to upload the dataset
dataset_file = upload_dataset()

import os

# List all files in the current directory (content)
print(os.listdir("/content"))

import os
import pandas as pd

# Define the path to the uploaded file
lexicon_path = "/content/mr.translit.sampled.train.tsv"

# Check if the file exists before loading it
if os.path.exists(lexicon_path):
    # Load the file into a DataFrame
    df = pd.read_csv(lexicon_path, sep='\t', header=None, names=['native', 'latin', 'attested'])

    # Show the first 5 rows
    print(df.head())
else:
    print(f"File '{lexicon_path}' not found. Please check the file path or upload it again.")

import numpy as np
import pandas as pd
import os
from tensorflow import keras
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense

# Parameters
batch_size = 64
epochs = 50
latent_dim = 256
num_samples = 50000

# Dataset path
data_path = "/content/mr.translit.sampled.train.tsv"

# Load and process the dataset
def load_data(path, num_samples):
    input_texts = []
    target_texts = []
    input_chars = set()
    target_chars = set()

    with open(path, "r", encoding="utf-8") as f:
        lines = f.read().splitlines()

    for line in lines[:min(num_samples, len(lines))]:
        try:
            native, latin, attested = line.split("\t")
        except ValueError:
            continue  # Skip malformed lines
        input_text = latin.strip()
        target_text = "\t" + native.strip() + "\n"  # Start and end tokens

        input_texts.append(input_text)
        target_texts.append(target_text)

        input_chars.update(input_text)
        target_chars.update(target_text)

    input_chars.add(" ")
    target_chars.add(" ")

    return input_texts, target_texts, sorted(input_chars), sorted(target_chars)

# Load dataset
input_texts, target_texts, input_characters, target_characters = load_data(data_path, num_samples)

# Token setup
num_encoder_tokens = len(input_characters)
num_decoder_tokens = len(target_characters)
max_encoder_seq_length = max(len(txt) for txt in input_texts)
max_decoder_seq_length = max(len(txt) for txt in target_texts)

print("Number of samples:", len(input_texts))
print("Number of unique input tokens:", num_encoder_tokens)
print("Number of unique output tokens:", num_decoder_tokens)
print("Max input length:", max_encoder_seq_length)
print("Max output length:", max_decoder_seq_length)

# Create token index maps
input_token_index = {char: i for i, char in enumerate(input_characters)}
target_token_index = {char: i for i, char in enumerate(target_characters)}

# Reverse maps for inference (optional)
reverse_input_char_index = {i: char for char, i in input_token_index.items()}
reverse_target_char_index = {i: char for char, i in target_token_index.items()}

# Vectorize data
encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype="float32")
decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype="float32")
decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype="float32")

for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):
    for t, char in enumerate(input_text):
        encoder_input_data[i, t, input_token_index[char]] = 1.0
    encoder_input_data[i, t + 1:, input_token_index[" "]] = 1.0  # Padding

    for t, char in enumerate(target_text):
        decoder_input_data[i, t, target_token_index[char]] = 1.0
        if t > 0:
            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0
    decoder_input_data[i, t + 1:, target_token_index[" "]] = 1.0
    decoder_target_data[i, t:, target_token_index[" "]] = 1.0

# Build the model
encoder_inputs = Input(shape=(None, num_encoder_tokens))
encoder_lstm = LSTM(latent_dim, return_state=True)
_, state_h, state_c = encoder_lstm(encoder_inputs)
encoder_states = [state_h, state_c]

decoder_inputs = Input(shape=(None, num_decoder_tokens))
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)

decoder_dense = Dense(num_decoder_tokens, activation="softmax")
decoder_outputs = decoder_dense(decoder_outputs)

model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Compile and train
model.compile(optimizer="rmsprop", loss="categorical_crossentropy", metrics=["accuracy"])

model.fit(
    [encoder_input_data, decoder_input_data],
    decoder_target_data,
    batch_size=batch_size,
    epochs=epochs,
    validation_split=0.2,
)

# Save the model
model.save("/content/s2s_model.keras")
print("Model saved to /content/s2s_model.keras")

import numpy as np
import keras

# Load the trained model
model = keras.models.load_model("s2s_model.keras")

# Get the encoder model
encoder_inputs = model.input[0]  # input_1
encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1
encoder_states = [state_h_enc, state_c_enc]
encoder_model = keras.Model(encoder_inputs, encoder_states)

# Get the decoder model
decoder_inputs = model.input[1]  # input_2
decoder_state_input_h = keras.Input(shape=(latent_dim,))
decoder_state_input_c = keras.Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
decoder_lstm = model.layers[3]
decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(
    decoder_inputs, initial_state=decoder_states_inputs
)
decoder_states = [state_h_dec, state_c_dec]
decoder_dense = model.layers[4]
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = keras.Model(
    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states
)

# Reverse-lookup token index to decode sequences back to something readable.
reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())
reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())

# Function to decode a sequence
def decode_sequence(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq, verbose=0)

    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    # Populate the first character of target sequence with the start character.
    target_seq[0, 0, target_token_index["\t"]] = 1.0

    # Sampling loop for a batch of sequences (simplified for batch size = 1).
    stop_condition = False
    decoded_sentence = ""
    while not stop_condition:
        # Get the output tokens and new states from the decoder model
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)

        # Sample a token (choose the token with the highest probability)
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        decoded_sentence += sampled_char

        # Exit condition: either hit max length or find stop character.
        if sampled_char == "\n" or len(decoded_sentence) > max_decoder_seq_length:
            stop_condition = True

        # Update the target sequence (of length 1)
        target_seq = np.zeros((1, 1, num_decoder_tokens))
        target_seq[0, 0, sampled_token_index] = 1.0

        # Update states
        states_value = [h, c]

    return decoded_sentence

# Example usage:
# Assuming `input_seq` is the one-hot encoded sequence of an input Latin sentence
# decoded_sentence = decode_sequence(input_seq)
# print(decoded_sentence)

import numpy as np

# Define max sequence length and number of tokens (adjust based on your data)
max_encoder_seq_length = 15  # Example maximum length of input sequences
num_encoder_tokens = len(input_token_index)  # Number of unique tokens in the input Latin vocabulary

def encode_input_text(input_text):
    # Create a zeroed-out input array
    encoder_input = np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype="float32")

    for t, char in enumerate(input_text):
        if char in input_token_index:
            encoder_input[0, t, input_token_index[char]] = 1.0
        else:
            print(f"Warning: character '{char}' not in training vocabulary.")

    # Ensure the rest of the input is padded with space token
    for t in range(len(input_text), max_encoder_seq_length):
        encoder_input[0, t, input_token_index[' ']] = 1.0  # Add space for padding

    return encoder_input

def decode_sequence(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model.predict(input_seq, verbose=0)

    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    target_seq[0, 0, target_token_index["\t"]] = 1.0  # Start token

    # Sampling loop for a batch of sequences (simplified for batch size = 1).
    stop_condition = False
    decoded_sentence = ""
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value, verbose=0)

        # Sample a token (choose the token with the highest probability)
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        decoded_sentence += sampled_char

        # Stop condition: stop if we hit the end token or exceed max length
        if sampled_char == "\n" or len(decoded_sentence) > max_decoder_seq_length:
            stop_condition = True

        # Update the target sequence (of length 1)
        target_seq = np.zeros((1, 1, num_decoder_tokens))
        target_seq[0, 0, sampled_token_index] = 1.0

        # Update states
        states_value = [h, c]

    return decoded_sentence

def translate_latin_to_devanagari():
    print("Enter a Latin word to translate (type 'exit' to quit):")
    while True:
        user_input = input("Latin word: ").strip()
        if user_input.lower() == "exit":
            break

        input_seq = encode_input_text(user_input)
        decoded_sentence = decode_sequence(input_seq)
        print("Devanagari transliteration:", decoded_sentence.strip())

# Call the function to start interactive translation
translate_latin_to_devanagari()

